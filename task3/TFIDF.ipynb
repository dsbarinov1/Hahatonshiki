{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"task-3-dataset.csv\") # Считываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3\n",
    "morph = pymorphy3.MorphAnalyzer(lang='ru')\n",
    "# Функция загрузки стопслов\n",
    "def downloads_():\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "# Функция обработки текта\n",
    "def foo(review):\n",
    "    # Обработка текста отзыва. Оставляем только буквы, приводим к нижнему регистру\n",
    "    review = review.lower()\n",
    "    review = review.replace('ё','е')\n",
    "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "    review = re.sub('[^а-я]', ' ', review)\n",
    "    # Отделяем слова\n",
    "    review = review.split()\n",
    "    # Избавляемся от стоп-слов(предлоги,союзы, частицы, не несущие семантической нагрузки)\n",
    "    review = [word for word in review if not word in set(stopwords.words('russian'))]\n",
    "    # Лемматизируем(для русского языка в явном виде нет, но пока тк)\n",
    "    # В явном виде лемматизации нет для русского языка,  SNOWBALL STEMMER как вариант\n",
    "    lemmatized_words = [morph.normal_forms(word)[0] for word in review]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим данные, пока нет тестовых\n",
    "from sklearn.model_selection import train_test_split\n",
    "df[\"разметка\"].loc[df[\"разметка\"]==\"+\"]=1\n",
    "df[\"разметка\"].loc[df[\"разметка\"]==\"-\"]=0\n",
    "df[\"отзывы\"] = df[\"отзывы\"].apply(foo)\n",
    "#dataset_train, dataset_test, train_data_label, test_data_label = train_test_split(df['отзывы'], df['разметка'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fayne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "downloads_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (210, 3884)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv=TfidfVectorizer(min_df=0.0,max_df=1.0,use_idf=True,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(df['отзывы'])\n",
    "#transformed test reviews\n",
    "print('Tfidf_train:',tv_train_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применим метод главных компонент для того, чтобы сократить вектор TF-IDF\n",
    "from sklearn.decomposition import PCA\n",
    "def reduce_dimensionality(X, n_components=4): # число кубит\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обрежем данные\n",
    "X = tv_train_reviews.copy()\n",
    "y = df['разметка']\n",
    "X_reduced = reduce_dimensionality(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qiskit\n",
    "from qiskit_aer import Aer\n",
    "from qiskit.circuit import QuantumCircuit, Parameter\n",
    "from qiskit.primitives import Sampler, Estimator\n",
    "#from qiskit.algorithms.optimizers import COBYLA\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.circuit.library import QNNCircuit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ┌─────────────┐                ░ ┌─┐         \n",
      "   q_0: ┤ Ry(theta_0) ├──■─────────────░─┤M├─────────\n",
      "        ├─────────────┤┌─┴─┐           ░ └╥┘┌─┐      \n",
      "   q_1: ┤ Ry(theta_1) ├┤ X ├──■────────░──╫─┤M├──────\n",
      "        ├─────────────┤└───┘┌─┴─┐      ░  ║ └╥┘┌─┐   \n",
      "   q_2: ┤ Ry(theta_2) ├─────┤ X ├──■───░──╫──╫─┤M├───\n",
      "        ├─────────────┤     └───┘┌─┴─┐ ░  ║  ║ └╥┘┌─┐\n",
      "   q_3: ┤ Ry(theta_3) ├──────────┤ X ├─░──╫──╫──╫─┤M├\n",
      "        └─────────────┘          └───┘ ░  ║  ║  ║ └╥┘\n",
      "meas: 4/══════════════════════════════════╩══╩══╩══╩═\n",
      "                                          0  1  2  3 \n"
     ]
    }
   ],
   "source": [
    "# Создадим квантовую схему\n",
    "n_qubits = 4 # Число кубит\n",
    "circuit = QuantumCircuit(n_qubits)\n",
    "# Параметры для схемы\n",
    "params = [Parameter(f'theta_{i}') for i in range(n_qubits * 2)]\n",
    "# Кодирование данных\n",
    "for i in range(n_qubits):\n",
    "    circuit.ry(params[i], i)\n",
    "# Параметризированные вращения\n",
    "for i in range(n_qubits - 1):\n",
    "    circuit.cx(i, i + 1)\n",
    "# Добавляем измерения\n",
    "circuit.measure_all()\n",
    "# Рисуем схему в текстовом формате\n",
    "print(circuit.draw(output='text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "\n",
    "num_inputs = 4\n",
    "feature_map = ZZFeatureMap(num_inputs)\n",
    "ansatz = RealAmplitudes(num_inputs,reps=1)\n",
    "\n",
    "circuit = QuantumCircuit(num_inputs)\n",
    "circuit.compose(feature_map, inplace=True)\n",
    "circuit.compose(ansatz, inplace=True)\n",
    "\n",
    "def parity(x):\n",
    "    return \"{:b}\".format(x).count(\"1\") % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fayne\\AppData\\Local\\Temp\\ipykernel_24296\\3788435028.py:4: DeprecationWarning: The class ``qiskit.primitives.sampler.Sampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Sampler` class is `StatevectorSampler`.\n",
      "  sampler = Sampler()\n",
      "C:\\Users\\fayne\\AppData\\Local\\Temp\\ipykernel_24296\\3788435028.py:5: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
      "  sampler_qnn = SamplerQNN(\n"
     ]
    }
   ],
   "source": [
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit.primitives import Sampler\n",
    "\n",
    "sampler = Sampler()\n",
    "sampler_qnn = SamplerQNN(\n",
    "    circuit=circuit,\n",
    "    input_params=feature_map.parameters,\n",
    "    weight_params=ansatz.parameters,\n",
    "    interpret=parity,\n",
    "    output_shape=2,\n",
    "    sampler=sampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.algorithms import NeuralNetworkClassifier\n",
    "classifier = NeuralNetworkClassifier(\n",
    "    neural_network=sampler_qnn,\n",
    "    loss=\"cross_entropy\",\n",
    "    one_hot=True,\n",
    ")\n",
    "classifier.fit(np.asarray(X_train), np.asarray(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
