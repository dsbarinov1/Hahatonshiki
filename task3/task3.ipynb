{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from bs4 import BeautifulSoup             \n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Tools for creating ngrams and vectorizing input data\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "\n",
    "# Configs\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "sns.set(style=\"whitegrid\")\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"task-3-dataset.csv\") # Считываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "# Функция загрузки стопслов\n",
    "def downloads_():\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "# Функция обработки текта\n",
    "def foo(review, morph):\n",
    "    # Обработка текста отзыва. Оставляем только буквы, приводим к нижнему регистру\n",
    "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "    review = re.sub('[^а-яА-Я]', ' ', review)\n",
    "    review = review.lower()\n",
    "    # Отделяем слова\n",
    "    review = review.split()\n",
    "    # Избавляемся от стоп-слов(предлоги,союзы, частицы, не несущие семантической нагрузки)\n",
    "    review = [word for word in review if not word in set(stopwords.words('russian'))]\n",
    "    # Лемматизируем(для русского языка в явном виде нет, но пока тк)\n",
    "    # В явном виде лемматизации нет для русского языка,  SNOWBALL STEMMER как вариант\n",
    "    lemmatized_words = [morph.normal_forms(word)[0] for word in review]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150    -\n",
       "96     +\n",
       "200    +\n",
       "68     +\n",
       "156    -\n",
       "      ..\n",
       "106    -\n",
       "14     -\n",
       "92     -\n",
       "179    -\n",
       "102    +\n",
       "Name: разметка, Length: 168, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Разделим данные, пока нет тестовых\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset_train, dataset_test, train_data_label, test_data_label = train_test_split(df['отзывы'], df['разметка'], test_size=0.2, random_state=42)\n",
    "train_data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fayne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Сформируем тестовый и трейновый словари\n",
    "corpus_train = []\n",
    "corpus_test  = []\n",
    "\n",
    "downloads_()\n",
    "for i in range(dataset_train.shape[0]):\n",
    "    review = dataset_train.iloc[i]\n",
    "    review = foo(review, morph)\n",
    "    corpus_train.append(review)\n",
    "\n",
    "for j in range(dataset_test.shape[0]):\n",
    "    review = dataset_test.iloc[j]\n",
    "    review = foo(review, morph)\n",
    "    corpus_test.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизуем с помощью TF-IDF \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "tfidf_vec_train = tfidf_vec.fit_transform(corpus_train)\n",
    "tfidf_vec_test = tfidf_vec.transform(corpus_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linear_svc = LinearSVC(C=0.5, random_state=42)\n",
    "linear_svc.fit(tfidf_vec_train, train_data_label)\n",
    "\n",
    "predict = linear_svc.predict(tfidf_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.93      0.93        28\n",
      "    Positive       0.86      0.86      0.86        14\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.89      0.89      0.89        42\n",
      "weighted avg       0.90      0.90      0.90        42\n",
      "\n",
      "Confusion Matrix: \n",
      " [[26  2]\n",
      " [ 2 12]]\n",
      "Accuracy: \n",
      " 0.9047619047619048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.68      0.78        28\n",
      "    Positive       0.57      0.86      0.69        14\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.74      0.77      0.73        42\n",
      "weighted avg       0.79      0.74      0.75        42\n",
      "\n",
      "Confusion Matrix: \n",
      " [[19  9]\n",
      " [ 2 12]]\n",
      "Accuracy: \n",
      " 0.7380952380952381\n"
     ]
    }
   ],
   "source": [
    "# Векторизуем с помощью другого векторизатора   \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec = CountVectorizer(ngram_range=(1, 3), binary=False)\n",
    "count_vec_train = count_vec.fit_transform(corpus_train)\n",
    "count_vec_test = count_vec.transform(corpus_test)\n",
    "\n",
    "linear_svc_count = LinearSVC(C=0.5, random_state=42, max_iter=5000)\n",
    "linear_svc_count.fit(count_vec_train, train_data_label)\n",
    "predict_count = linear_svc_count.predict(count_vec_test)\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict_count,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_count))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.68      0.78        28\n",
      "    Positive       0.57      0.86      0.69        14\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.74      0.77      0.73        42\n",
      "weighted avg       0.79      0.74      0.75        42\n",
      "\n",
      "Confusion Matrix: \n",
      " [[19  9]\n",
      " [ 2 12]]\n",
      "Accuracy: \n",
      " 0.7380952380952381\n"
     ]
    }
   ],
   "source": [
    "# И снова другой векторизатор\n",
    "ind_vec = CountVectorizer(ngram_range=(1, 3), binary=True)\n",
    "ind_vec_train = ind_vec.fit_transform(corpus_train)\n",
    "ind_vec_test = ind_vec.transform(corpus_test)\n",
    "\n",
    "linear_svc_ind = LinearSVC(C=0.5, random_state=42)\n",
    "linear_svc_ind.fit(ind_vec_train, train_data_label)\n",
    "predict_ind = linear_svc_ind.predict(ind_vec_test)\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict_ind,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_ind))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 762) (42, 762)\n"
     ]
    }
   ],
   "source": [
    "# TF_IDF дал лучший результат, используем его, добаим наивный байесовский классификатор\n",
    "tfidf_vec_NB = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf_vec_train_NB = tfidf_vec_NB.fit_transform(corpus_train)\n",
    "\n",
    "tfidf_vec_test_NB = tfidf_vec_NB.transform(corpus_test)\n",
    "\n",
    "print(tfidf_vec_train_NB.toarray().shape, tfidf_vec_test_NB.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fayne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:776: UserWarning: k=50000 is greater than n_features=762. All the features will be returned.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "ch2 = SelectKBest(chi2, k=50000)\n",
    "tfidf_vec_train_NB = ch2.fit_transform(tfidf_vec_train_NB, train_data_label)\n",
    "tfidf_vec_test_NB  = ch2.transform(tfidf_vec_test_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.96      0.95        28\n",
      "    Positive       0.92      0.86      0.89        14\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.93      0.91      0.92        42\n",
      "weighted avg       0.93      0.93      0.93        42\n",
      "\n",
      "Confusion Matrix: \n",
      " [[27  1]\n",
      " [ 2 12]]\n",
      "Accuracy: \n",
      " 0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vec_NB.get_feature_names_out()\n",
    "feature_names = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]\n",
    "feature_names = np.asarray(feature_names)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "multi_clf = MultinomialNB()\n",
    "multi_clf.fit(tfidf_vec_train_NB, train_data_label)\n",
    "predict_NB = multi_clf.predict(tfidf_vec_test_NB)\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict_NB,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_NB))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.89      0.91        28\n",
      "    Positive       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.86      0.88      0.87        42\n",
      "weighted avg       0.88      0.88      0.88        42\n",
      "\n",
      "Confusion Matrix: \n",
      " [[25  3]\n",
      " [ 2 12]]\n",
      "Accuracy: \n",
      " 0.8809523809523809\n"
     ]
    }
   ],
   "source": [
    "count_vec_NB = CountVectorizer(ngram_range=(1, 3), binary=False)\n",
    "count_vec_train_NB = count_vec_NB.fit_transform(corpus_train)\n",
    "count_vec_test_NB = count_vec_NB.transform(corpus_test)\n",
    "\n",
    "multi_clf_count = MultinomialNB()\n",
    "multi_clf_count.fit(count_vec_train_NB, train_data_label)\n",
    "predict_NB_count = multi_clf_count.predict(count_vec_test_NB)\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict_NB_count,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_NB_count))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_NB_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем теперь LSTM НАХУЙ\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Masking, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***ВАРИАНТ РЕШЕНИЯ С LSTM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 200\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "df = pd.read_csv('task-3-dataset.csv')\n",
    "df[\"разметка\"].loc[df[\"разметка\"]==\"+\"]=1\n",
    "df[\"разметка\"].loc[df[\"разметка\"]==\"-\"]=0\n",
    "X_data = df.drop(['разметка'],axis=1)\n",
    "y_data = df['отзывы']\n",
    "train, test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "train.columns = ['отзывы']\n",
    "test.columns = ['отзывы']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train['отзывы'])\n",
    "X_train_token = tokenizer.texts_to_sequences(train['отзывы'])\n",
    "tokenizer.fit_on_texts(test['отзывы'])\n",
    "X_test_token = tokenizer.texts_to_sequences(test['отзывы'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train_token, maxlen=maxlen, padding='post')\n",
    "X_test  = pad_sequences(X_test_token, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data_label.copy()\n",
    "y_train = y_train.replace('-',0)\n",
    "y_train = y_train.replace('+',1)\n",
    "y_test  = test_data_label.copy()\n",
    "y_test = y_test.replace('-',0)\n",
    "y_test = y_test.replace('+',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Embedding(max_features, 64, mask_zero=True),\n",
    "                    Bidirectional(LSTM(64, dropout=0.2)),\n",
    "                    Dense(64, activation='sigmoid'),\n",
    "                    Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.4426 - loss: 8.9847 - val_accuracy: 0.3333 - val_loss: 10.7454\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.4392 - loss: 9.0384 - val_accuracy: 0.3333 - val_loss: 10.7454\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4312 - loss: 9.1674 - val_accuracy: 0.3333 - val_loss: 10.7454\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4426 - loss: 8.9847 - val_accuracy: 0.3333 - val_loss: 10.7454\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4586 - loss: 8.7268 - val_accuracy: 0.3333 - val_loss: 10.7454\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.4599 - loss: 8.7053 - val_accuracy: 0.3333 - val_loss: 10.7454\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.4679 - loss: 8.5764 - val_accuracy: 0.3333 - val_loss: 10.7454\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4472 - loss: 8.9095 - val_accuracy: 0.3333 - val_loss: 10.7454\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.4246 - loss: 9.2748 - val_accuracy: 0.3333 - val_loss: 10.7454\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.4479 - loss: 8.8987 - val_accuracy: 0.3333 - val_loss: 10.7454\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=50,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
