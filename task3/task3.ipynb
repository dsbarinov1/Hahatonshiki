{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fayne\\AppData\\Local\\Temp\\ipykernel_7288\\909426180.py:23: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from bs4 import BeautifulSoup             \n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Tools for creating ngrams and vectorizing input data\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "\n",
    "# Configs\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.style.use('seaborn')\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"task-3-dataset.csv\") # Считываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"разметка\"].loc[df[\"разметка\"]==\"+\"]=1.0\n",
    "df[\"разметка\"].loc[df[\"разметка\"]==\"-\"]=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "# Функция загрузки стопслов\n",
    "def downloads_():\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "# Функция обработки текта\n",
    "def foo(review, morph):\n",
    "    # Обработка текста отзыва. Оставляем только буквы, приводим к нижнему регистру\n",
    "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "    review = re.sub('[^а-яА-Я]', ' ', review)\n",
    "    review = review.lower()\n",
    "    # Отделяем слова\n",
    "    review = review.split()\n",
    "    # Избавляемся от стоп-слов(предлоги,союзы, частицы, не несущие семантической нагрузки)\n",
    "    review = [word for word in review if not word in set(stopwords.words('russian'))]\n",
    "    # Лемматизируем(для русского языка в явном виде нет, но пока тк)\n",
    "    # В явном виде лемматизации нет для русского языка,  SNOWBALL STEMMER как вариант\n",
    "    lemmatized_words = [morph.normal_forms(word)[0] for word in review]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим данные, пока нет тестовых\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset_train, dataset_test, train_data_label, test_data_label = train_test_split(df['отзывы'], df['разметка'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fayne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Сформируем тестовый и трейновый словари\n",
    "corpus_train = []\n",
    "corpus_test  = []\n",
    "\n",
    "downloads_()\n",
    "for i in range(dataset_train.shape[0]):\n",
    "    review = dataset_train.iloc[i]\n",
    "    review = foo(review, morph)\n",
    "    corpus_train.append(review)\n",
    "\n",
    "for j in range(dataset_test.shape[0]):\n",
    "    review = dataset_test.iloc[j]\n",
    "    review = foo(review, morph)\n",
    "    corpus_test.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизуем с помощью TF-IDF \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "tfidf_vec_train = tfidf_vec.fit_transform(corpus_train)\n",
    "tfidf_vec_test = tfidf_vec.transform(corpus_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linear_svc = LinearSVC(C=0.5, random_state=42)\n",
    "linear_svc.fit(tfidf_vec_train, train_data_label)\n",
    "\n",
    "predict = linear_svc.predict(tfidf_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.93      0.93        28\n",
      "    Positive       0.86      0.86      0.86        14\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.89      0.89      0.89        42\n",
      "weighted avg       0.90      0.90      0.90        42\n",
      "\n",
      "Confusion Matrix: \n",
      " [[26  2]\n",
      " [ 2 12]]\n",
      "Accuracy: \n",
      " 0.9047619047619048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.68      0.78        28\n",
      "    Positive       0.57      0.86      0.69        14\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.74      0.77      0.73        42\n",
      "weighted avg       0.79      0.74      0.75        42\n",
      "\n",
      "Confusion Matrix: \n",
      " [[19  9]\n",
      " [ 2 12]]\n",
      "Accuracy: \n",
      " 0.7380952380952381\n"
     ]
    }
   ],
   "source": [
    "# Векторизуем с помощью другого векторизатора   \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec = CountVectorizer(ngram_range=(1, 3), binary=False)\n",
    "count_vec_train = count_vec.fit_transform(corpus_train)\n",
    "count_vec_test = count_vec.transform(corpus_test)\n",
    "\n",
    "linear_svc_count = LinearSVC(C=0.5, random_state=42, max_iter=5000)\n",
    "linear_svc_count.fit(count_vec_train, train_data_label)\n",
    "predict_count = linear_svc_count.predict(count_vec_test)\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict_count,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_count))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.68      0.78        28\n",
      "    Positive       0.57      0.86      0.69        14\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.74      0.77      0.73        42\n",
      "weighted avg       0.79      0.74      0.75        42\n",
      "\n",
      "Confusion Matrix: \n",
      " [[19  9]\n",
      " [ 2 12]]\n",
      "Accuracy: \n",
      " 0.7380952380952381\n"
     ]
    }
   ],
   "source": [
    "# И снова другой векторизатор\n",
    "ind_vec = CountVectorizer(ngram_range=(1, 3), binary=True)\n",
    "ind_vec_train = ind_vec.fit_transform(corpus_train)\n",
    "ind_vec_test = ind_vec.transform(corpus_test)\n",
    "\n",
    "linear_svc_ind = LinearSVC(C=0.5, random_state=42)\n",
    "linear_svc_ind.fit(ind_vec_train, train_data_label)\n",
    "predict_ind = linear_svc_ind.predict(ind_vec_test)\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict_ind,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_ind))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 762) (42, 762)\n"
     ]
    }
   ],
   "source": [
    "# TF_IDF дал лучший результат, используем его, добаим наивный байесовский классификатор\n",
    "tfidf_vec_NB = TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf_vec_train_NB = tfidf_vec_NB.fit_transform(corpus_train)\n",
    "\n",
    "tfidf_vec_test_NB = tfidf_vec_NB.transform(corpus_test)\n",
    "\n",
    "print(tfidf_vec_train_NB.toarray().shape, tfidf_vec_test_NB.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fayne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:776: UserWarning: k=50000 is greater than n_features=762. All the features will be returned.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "ch2 = SelectKBest(chi2, k=50000)\n",
    "tfidf_vec_train_NB = ch2.fit_transform(tfidf_vec_train_NB, train_data_label)\n",
    "tfidf_vec_test_NB  = ch2.transform(tfidf_vec_test_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.96      0.95        28\n",
      "    Positive       0.92      0.86      0.89        14\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.93      0.91      0.92        42\n",
      "weighted avg       0.93      0.93      0.93        42\n",
      "\n",
      "Confusion Matrix: \n",
      " [[27  1]\n",
      " [ 2 12]]\n",
      "Accuracy: \n",
      " 0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vec_NB.get_feature_names_out()\n",
    "feature_names = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]\n",
    "feature_names = np.asarray(feature_names)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "multi_clf = MultinomialNB()\n",
    "multi_clf.fit(tfidf_vec_train_NB, train_data_label)\n",
    "predict_NB = multi_clf.predict(tfidf_vec_test_NB)\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict_NB,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_NB))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.89      0.91        28\n",
      "    Positive       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.86      0.88      0.87        42\n",
      "weighted avg       0.88      0.88      0.88        42\n",
      "\n",
      "Confusion Matrix: \n",
      " [[25  3]\n",
      " [ 2 12]]\n",
      "Accuracy: \n",
      " 0.8809523809523809\n"
     ]
    }
   ],
   "source": [
    "count_vec_NB = CountVectorizer(ngram_range=(1, 3), binary=False)\n",
    "count_vec_train_NB = count_vec_NB.fit_transform(corpus_train)\n",
    "count_vec_test_NB = count_vec_NB.transform(corpus_test)\n",
    "\n",
    "multi_clf_count = MultinomialNB()\n",
    "multi_clf_count.fit(count_vec_train_NB, train_data_label)\n",
    "predict_NB_count = multi_clf_count.predict(count_vec_test_NB)\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(test_data_label, predict_NB_count,target_names=['Negative','Positive']))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict_NB_count))\n",
    "print(\"Accuracy: \\n\", accuracy_score(test_data_label, predict_NB_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем теперь LSTM НАХУЙ\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Masking, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***ВАРИАНТ РЕШЕНИЯ С LSTM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 200\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "\n",
    "\n",
    "X_data = df.drop(['разметка'],axis=1)\n",
    "y_data = df['отзывы']\n",
    "train, test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "train.columns = ['отзывы']\n",
    "test.columns = ['отзывы']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train['отзывы'])\n",
    "X_train_token = tokenizer.texts_to_sequences(train['отзывы'])\n",
    "tokenizer.fit_on_texts(test['отзывы'])\n",
    "X_test_token = tokenizer.texts_to_sequences(test['отзывы'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train_token, maxlen=maxlen, padding='post')\n",
    "X_test  = pad_sequences(X_test_token, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data_label.copy()\n",
    "y_test  = test_data_label.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Embedding(max_features, 64, mask_zero=True),\n",
    "                    Bidirectional(LSTM(64, dropout=0.2)),\n",
    "                    Dense(64, activation='sigmoid'),\n",
    "                    Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[217], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fayne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\fayne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=50,\n",
    "                    epochs=3,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
